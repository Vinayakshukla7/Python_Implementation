{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Scrapy ?\n",
    "Scrapy is a fast, open-source web crawling framework written in Python,\n",
    "Used for extracting the data you need from websites.In a fast, simple, yet extensible way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use scrapy ?\n",
    "It is easier to build and scale large crawling projects.<br>\n",
    "It has a built-in mechanism called Selectors, for extracting the data from websites.<br>\n",
    "It handles the requests asynchronously and it is fast.<br>\n",
    "Scrapy generates feed exports in formats such as JSON, CSV, and XML.<br>\n",
    "Scrapy has built-in support for selecting and extracting data from sources either by XPath or CSS expressions.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a project"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy startproject first_scrapy                                          #first_scrapy is our project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will create a first_scrapy directory with the following contents:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first_scrapy/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    first_scrapy/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first Spider\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy genspider quotes quotes.toscrape.com        #this will create spider for quotes.toscrape.com     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy                                    #this is our spider created\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"                                # identifies the Spider    \n",
    "\n",
    "    def start_requests(self):                  #  return a list of requests on which the Spider will begin to crawl from\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)        #callback used to call parse method for each url\n",
    "\n",
    "    def parse(self, response):          #method that will be called to handle the response downloaded for each of the requests made. \n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Saved file {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run our spider"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy crawl quotes              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that two new files have been created: quotes-1.html and quotes-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):                   #start_url directly can be used to avoid itterations\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. The Scrapy shell is an interactive shell where you can try and debug your scraping code very quickly, without having to run the spider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell \"http://quotes.toscrape.com/page/1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run above code ,you get the available scrapy objects you can work on"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ ... Scrapy log here ... ]\n",
    "2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET http://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 http://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>\n",
    "[s] Useful shortcuts:\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n",
    "[s]   view(response)    View response in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the shell, \n",
    "you can try selecting elements using CSS with the response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of running response.css('title') is a list-like object called SelectorList.<br>\n",
    "To extract the text from the title above, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').getall()             # getall() return list of all .           \n",
    "#['Quotes to Scrape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title').getall()                     #::text\n",
    "#['<title>Quotes to Scrape</title>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').get()               #single search\n",
    "#'Quotes to Scrape'\n",
    "\n",
    "response.css('title::text')[0].get()\n",
    "#'Quotes to Scrape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Besides the getall() and get() methods, you can also use the re() method to extract using regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').re(r'Quotes.*')\n",
    "#['Quotes to Scrape']\n",
    "\n",
    "response.css('title::text').re(r'Q\\w+')\n",
    "#['Quotes']\n",
    "\n",
    "response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "#['Quotes', 'Scrape']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath: another way to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath('//title')\n",
    "#[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "response.xpath('//title/text()').get()\n",
    "#'Quotes to Scrape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css(\"div.quote\")\n",
    "#[<Selector xpath=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    "# <Selector xpath=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = response.css(\"div.quote\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract text, author and the tags from that quote using the quote object we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = quote.css(\"span.text::text\").get()\n",
    "print(text)\n",
    "#'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "\n",
    "author = quote.css(\"small.author::text\").get()\n",
    "print(author)\n",
    "#'Albert Einstein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "print(tags)\n",
    "#['change', 'deep-thoughts', 'thinking', 'world']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put them all together in dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in response.css(\"div.quote\"):\n",
    "    text = quote.css(\"span.text::text\").get()\n",
    "    author = quote.css(\"small.author::text\").get()\n",
    "    tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "    print(dict(text=text, author=author, tags=tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data in our spider\n",
    "integrate the extraction logic above into our spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl quotes -O quotes.json        #.csv ,.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -O command-line switch overwrites any existing file; use -o instead to append new content to any existing file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Scrape product's name,price,link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject whiskyscrapper                              #whiskyscrapper is name of project                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Scrapy shell\n",
    "when we are using scrapy shell everything is going to be save in response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd whiskyscrapper  #get to directory\n",
    " \n",
    "scrapy shell      #enable shell       scrapy shell \"site.com\"\n",
    "\n",
    "fetch('https://www.whiskyshop.com/scotch-whisky')          #fetch method to get the link which returns reponse object\n",
    "\n",
    "#response shows get<200> then good to go else check url once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection of website\n",
    "Will help to get the elements from which we will scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('div.product-item-info')     #selector used to select specify tag\n",
    "\n",
    "response.css('div.product-item-info').get()       #get() used to get that content ,givesonly first found element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('div.product-item-info').extract()  #it will give the whole tag in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=response.css('div.product-item-info')\n",
    "len(product)                               #100 i.e... no of producct per page         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product.css('a.product-item-link::text').getall()     #will give name of all 100 products\n",
    "\n",
    "product.css('span.price::text').getall()           #will give prices of all 100 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product.css('a.product-item-link').attrib['href']         #links of product items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spider\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider whiskyspider www.whiskyshop.com/scotch-whisky         #scrapy spider genspider class_name link_given ,will create spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes made in Spider \n",
    "i.e.. in whiskyspider.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class WhiskyspiderSpider(scrapy.Spider):\n",
    "    name = 'whisky'\n",
    "    allowed_domains = ['www.whiskyshop.com/scotch-whisky']\n",
    "    start_urls = ['http://www.whiskyshop.com/scotch-whisky/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for product in response.css('div.product-item-info'):\n",
    "            try:    \n",
    "                yield{\n",
    "                    'name': product.css('a.product-item-link::text').get(),\n",
    "                    'price': product.css('span.price::text').get().replace('£',''),\n",
    "                    'link':  product.css('a.product-item-link').attrib['href'] ,\n",
    "                }\n",
    "            except:\n",
    "                yield{\n",
    "                    'name': product.css('a.product-item-link::text').get(),\n",
    "                    'price': 'Not Available',\n",
    "                    'link':  product.css('a.product-item-link').attrib['href'] ,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Crawl the Spider\n",
    "Basically it means to run the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl whisky                                                          #scrapy crawl spider_name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert output into .json format\n",
    "scrapy crawl whisky -O whisky.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries to bypass\n",
    "#scrapy-user-agents\n",
    "#scrapy-proxy-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback is not working\n",
    "\"\"\"response.css('a.action.next')\n",
    "response.css('a.action.next').attrib['href'] \n",
    "\n",
    "\n",
    "next_pg=response.css('a.action.next').attrib['href']        \n",
    "    if next_pg is not None:                                        #if next page is there\n",
    "        yield response.follow(next_pg, callback=self.parse)        #follow will take back to next page and callback used to call parse method for next_page also\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
